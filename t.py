import pandas as pd
import os
import warnings
import glob
import sys

# Fix console encoding for Windows
if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')

warnings.filterwarnings("ignore", category=UserWarning, module="openpyxl")

# =============================================================================
# CONFIGURATION
# =============================================================================

# Define your directories
source_directory = os.path.dirname(os.path.abspath(__file__))
destination_directory = os.path.join(source_directory, 'Checklist')
os.makedirs(destination_directory, exist_ok=True)

# Define files to convert with their sheet names
files_to_convert = {
    'NOUVEAU FAC PERSPECTIVIA.xlsx': None,  # None = auto-detect first sheet
}


# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def replace_unsupported_characters(text):
    """Replace unsupported Unicode characters with standard equivalents"""
    if not isinstance(text, str):
        return text
    return text.replace('\u2013', '-').replace('\u2014', '-').replace('\u2026', '...')


def cleanse_data(df):
    """Clean DataFrame by replacing Excel errors with NA and cleaning text"""
    # Replace Excel error values
    df.replace(['#N/A', '#VALUE!', '#REF!', '#DIV/0!', '#NUM!', '#NAME?', '#NULL!'],
               pd.NA, inplace=True)

    # Apply character replacement to all string columns
    for col in df.select_dtypes(include=['object']).columns:
        df[col] = df[col].apply(replace_unsupported_characters)

    return df


# =============================================================================
# STEP 1 & 2: CONVERT XLSX TO CSV WITH PROPER ENCODING
# =============================================================================

def convert_xlsx_to_csv(files_dict, source_dir, dest_dir, encoding='utf_8_sig'):
    """Convert Excel files to CSV with proper encoding"""
    converted_files = []

    for file, sheet in files_dict.items():
        file_path = os.path.join(source_dir, file)

        try:
            # Read Excel file
            if sheet is None:
                # Auto-detect: read first sheet
                df = pd.read_excel(file_path, sheet_name=0)
            else:
                df = pd.read_excel(file_path, sheet_name=sheet)

            # Clean the data
            df_clean = cleanse_data(df)

            # Generate output filename
            csv_file_name = file.replace('.xlsx', '.csv')
            output_file_path = os.path.join(dest_dir, csv_file_name)

            # Save to CSV with proper encoding and semicolon separator
            df_clean.to_csv(output_file_path, index=False, encoding=encoding, sep=';')

            converted_files.append(output_file_path)
            print(f"‚úì Converted: {file} ‚Üí {csv_file_name}")

        except Exception as e:
            import traceback
            print(f"‚úó Error processing {file}: {e}")
            traceback.print_exc()

    return converted_files


# =============================================================================
# STEP 3: SEGMENT DATA BY VAGUE AND √âTAT
# =============================================================================

def segment_by_vague(csv_files, dest_dir, vague_column='Vague', etat_column='√âTAT'):
    """
    Create separate CSV files per vague with all data intact
    Categorizes data using both Vague and √âtat columns
    """

    for csv_file in csv_files:
        try:
            # Read the CSV file with semicolon separator
            df = pd.read_csv(csv_file, encoding='utf_8_sig', sep=';')

            # Check if required columns exist
            if vague_column not in df.columns:
                print(f"‚ö† Warning: '{vague_column}' column not found in {csv_file}")
                continue

            if etat_column not in df.columns:
                print(f"‚ö† Warning: '{etat_column}' column not found in {csv_file}")
                # Continue anyway if √âtat is missing

            # Check for payment columns
            payment_columns = ['PAIEMENT 1', 'PAIEMENT 2', 'PAIEMENT 3']
            available_payments = [col for col in payment_columns if col in df.columns]

            if not available_payments:
                print(f"‚ö† Warning: No payment columns found in {csv_file}")

            # Get base filename
            base_name = os.path.basename(csv_file).replace('.csv', '')

            # Get unique vagues (cycles)
            unique_vagues = df[vague_column].dropna().unique()

            print(f"\nüìä Processing: {base_name}")
            print(f"   Found {len(unique_vagues)} unique vagues/cycles")

            # Create a CSV for each vague
            for vague in unique_vagues:
                # Filter data for this vague
                df_vague = df[df[vague_column] == vague].copy()

                # Generate output filename
                vague_safe = str(vague).replace('/', '_').replace('\\', '_')
                output_filename = f"{base_name}_Vague_{vague_safe}.csv"
                output_path = os.path.join(dest_dir, output_filename)

                # Save to CSV (all data untransformed) with semicolon separator
                df_vague.to_csv(output_path, index=False, encoding='utf_8_sig', sep=';')

                # Show summary
                if etat_column in df.columns:
                    etat_counts = df_vague[etat_column].value_counts()
                    print(f"   ‚úì Vague '{vague}': {len(df_vague)} rows")
                    print(f"      √âtats: {dict(etat_counts)}")
                else:
                    print(f"   ‚úì Vague '{vague}': {len(df_vague)} rows")

            # Define detailed status categories
            status_categories = {
                'R√©√©l': ['PEC accord√©', 'Factur√©', 'Encaiss√©'],
                'Pr√©visionnel': ['ATT DE PEC', 'ATT DE PEC R1', 'ATT DE PEC R2', 'ATT DE PEC R3',
                                 'ATT DE PEC R4', 'ATT DE PEC R5', 'ATT DE PEC R6', 'ATT DE PEC R7',
                                 'ATT DE PEC R8', 'ATT DE PEC R9', 'ATT DE PEC R10', 'ATT DE PEC R11',
                                 'ATT DE PEC R12'],
                'Potentiel': ['Signature bi-parti √† d√©poser', 'D√©p√¥t brouillon', 'Manque info', 'Manque documents']
            }

            # Map main categories to their detail columns
            detail_columns = {
                'R√©√©l': 'R√©√©l',
                'Pr√©visionnel': 'Pr√©visionnel',
                'Potentiel': 'Potentiel'
            }

            # Create enhanced summary with payment totals - RENAMED FILE
            if etat_column in df.columns:
                summary_data = []

                for vague in unique_vagues:
                    df_vague = df[df[vague_column] == vague]

                    for etat in df_vague[etat_column].dropna().unique():
                        df_subset = df_vague[df_vague[etat_column] == etat]

                        row = {
                            vague_column: vague,
                            etat_column: etat,
                            'Count': len(df_subset)
                        }

                        # Add payment totals for each payment column
                        for payment_col in available_payments:
                            # Convert to numeric, handling errors
                            payment_values = pd.to_numeric(df_subset[payment_col], errors='coerce')
                            total = payment_values.sum()
                            row[f'Total_{payment_col}'] = total

                        # Calculate total across all payments
                        if available_payments:
                            total_all_payments = sum(
                                pd.to_numeric(df_subset[col], errors='coerce').sum()
                                for col in available_payments
                            )
                            row['Total_All_Payments'] = total_all_payments

                        # Initialize all detailed status columns to 0
                        for category, statuses in status_categories.items():
                            for status in statuses:
                                row[status] = 0

                        # Count occurrences of each detailed status from the appropriate column
                        if etat in detail_columns and detail_columns[etat] in df.columns:
                            detail_col = detail_columns[etat]
                            # Count the detailed statuses from the specific column
                            for detailed_status in df_subset[detail_col].value_counts().items():
                                status_name, count = detailed_status
                                # Only add if this status is in our predefined list
                                if status_name in row:
                                    row[status_name] = count

                        # Calculate CA based on √âTAT
                        if etat in ['R√©el', 'R√©√©l']:
                            row['CA_R√©√©l'] = total_all_payments
                            row['CA_Pr√©visionnel'] = 0
                            row['CA_Potentiel'] = 0
                        elif etat == 'Pr√©visionnel':
                            row['CA_R√©√©l'] = 0
                            row['CA_Pr√©visionnel'] = len(df_subset) * 3100
                            row['CA_Potentiel'] = 0
                        elif etat == 'Potentiel':
                            row['CA_R√©√©l'] = 0
                            row['CA_Pr√©visionnel'] = 0
                            row['CA_Potentiel'] = len(df_subset) * 3100
                        else:
                            row['CA_R√©√©l'] = 0
                            row['CA_Pr√©visionnel'] = 0
                            row['CA_Potentiel'] = 0

                        summary_data.append(row)

                summary = pd.DataFrame(summary_data)

                # RENAMED: Save as "Donn√©es_Transform√©es" instead of "Summary_Vague_√âtat"
                summary_path = os.path.join(dest_dir, f"Donn√©es_Transform√©es.csv")
                summary.to_csv(summary_path, index=False, encoding='utf_8_sig', sep=';')
                print(f"   ‚úì Created summary: Donn√©es_Transform√©es.csv")

                # Create Intermediary Status CSV - only detailed status columns with values > 0
                intermediary_columns = ['Vague', '√âTAT', 'Count']

                # Add all detailed status columns
                for category, statuses in status_categories.items():
                    for status in statuses:
                        intermediary_columns.append(status)

                # Filter to only include these columns and rows where at least one status > 0
                intermediary_df = summary[intermediary_columns].copy()

                # Create a mask for rows where any detailed status column > 0
                status_cols = [col for col in intermediary_columns if col not in ['Vague', '√âTAT', 'Count']]
                mask = (intermediary_df[status_cols] > 0).any(axis=1)
                intermediary_df_filtered = intermediary_df[mask]

                # Save intermediary status CSV
                intermediary_path = os.path.join(dest_dir, f"Statuts_Interm√©diaires.csv")
                intermediary_df_filtered.to_csv(intermediary_path, index=False, encoding='utf_8_sig', sep=';')
                print(f"   ‚úì Created intermediary status file: Statuts_Interm√©diaires.csv")

                # Create PROMO breakdown CSV - only R√©√©l category with PROMO counts
                promo_data = []

                for vague in unique_vagues:
                    df_vague = df[df[vague_column] == vague]

                    # Only for R√©√©l category
                    df_reel = df_vague[df_vague[etat_column] == 'R√©√©l']

                    if not df_reel.empty and 'PROMO' in df.columns:
                        # Clean PROMO names: strip whitespace and normalize
                        df_reel_clean = df_reel.copy()
                        df_reel_clean['PROMO'] = df_reel_clean['PROMO'].astype(str).str.strip().str.upper()

                        # Count formations per PROMO
                        promo_counts = df_reel_clean['PROMO'].value_counts()

                        for promo, count in promo_counts.items():
                            if pd.notna(promo) and promo != 'NAN':  # Skip NaN values
                                promo_data.append({
                                    'Vague': vague,
                                    'PROMO': promo,
                                    'Count': count
                                })

                if promo_data:
                    promo_df = pd.DataFrame(promo_data)
                    promo_path = os.path.join(dest_dir, f"PROMO_R√©√©l_par_Vague.csv")
                    promo_df.to_csv(promo_path, index=False, encoding='utf_8_sig', sep=';')
                    print(f"   ‚úì Created PROMO breakdown file (R√©√©l only): PROMO_R√©√©l_par_Vague.csv")

                    # Display PROMO summary
                    print(f"\nüìö PROMO Summary (R√©√©l):")
                    for vague in unique_vagues:
                        df_vague_promo = promo_df[promo_df['Vague'] == vague]
                        total = df_vague_promo['Count'].sum()
                        if total > 0:
                            print(f"{vague}: {total} formations r√©elles")
                            promos = df_vague_promo.nlargest(5, 'Count')
                            for _, row in promos.iterrows():
                                print(f"  - {row['PROMO']}: {row['Count']}")
                else:
                    print(f"   ‚ö† Warning: No PROMO data found or PROMO column missing")

                # Display summary table
                print(f"\nüìà Payment Summary by Vague and √âtat:")
                print(summary.to_string(index=False))

                # Display CA summary
                print(f"\nüí∞ CA Summary:")
                print(f"Total CA R√©√©l: {summary['CA_R√©√©l'].sum():,.2f}‚Ç¨")
                print(f"Total CA Pr√©visionnel: {summary['CA_Pr√©visionnel'].sum():,.2f}‚Ç¨")
                print(f"Total CA Potentiel: {summary['CA_Potentiel'].sum():,.2f}‚Ç¨")
                print(
                    f"Total CA Global: {(summary['CA_R√©√©l'].sum() + summary['CA_Pr√©visionnel'].sum() + summary['CA_Potentiel'].sum()):,.2f}‚Ç¨")

                # Check for Potentiel/Pr√©visionnel with payments (should be zero)
                non_reel_states = ['Potentiel', 'Pr√©visionnel']
                for state in non_reel_states:
                    state_data = summary[summary[etat_column] == state]
                    if not state_data.empty and 'Total_All_Payments' in state_data.columns:
                        total = state_data['Total_All_Payments'].sum()
                        if total > 0:
                            print(f"\n‚ö†Ô∏è  WARNING: Found {total}‚Ç¨ in payments for '{state}' (expected 0)")
                        else:
                            print(f"\n‚úì Confirmed: No payments for '{state}' (as expected)")

                # =============================================================================
                # CREATE CHECKLISTS
                # =============================================================================

                print(f"\nüìã Creating checklists...")

                # Use the same directory as destination_directory
                checklist_dir = destination_directory

                # Checklist 1: Cindy - PEC accord√© (from R√©√©l column)
                if 'R√©√©l' in df.columns:
                    df_cindy = df[df['R√©√©l'] == 'PEC accord√©'].copy()
                    if not df_cindy.empty:
                        cindy_path = os.path.join(checklist_dir, 'checklist_cindy.csv')
                        df_cindy.to_csv(cindy_path, index=False, encoding='utf_8_sig', sep=';')
                        print(f"   ‚úì Checklist Cindy: {len(df_cindy)} formations (PEC accord√©)")
                    else:
                        print(f"   ‚ö† No formations found with 'PEC accord√©' status")

                # Checklist 2: Admin d√©p√¥t initial - Signature bi-parti √† d√©poser (from Potentiel column)
                if 'Potentiel' in df.columns:
                    df_depot_initial = df[df['Potentiel'] == 'Signature bi-parti √† d√©poser'].copy()
                    if not df_depot_initial.empty:
                        depot_initial_path = os.path.join(checklist_dir, 'checklist_admin_d√©p√¥t_initial.csv')
                        df_depot_initial.to_csv(depot_initial_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist Admin D√©p√¥t Initial: {len(df_depot_initial)} formations (Signature bi-parti √† d√©poser)")
                    else:
                        print(f"   ‚ö† No formations found with 'Signature bi-parti √† d√©poser' status")

                # Checklist 3: Admin v√©rifier d√©p√¥t - D√©p√¥t brouillon (from Potentiel column)
                if 'Potentiel' in df.columns:
                    df_verifier_depot = df[df['Potentiel'] == 'D√©p√¥t brouillon'].copy()
                    if not df_verifier_depot.empty:
                        verifier_depot_path = os.path.join(checklist_dir, 'checklist_admin_v√©rifier_d√©p√¥t.csv')
                        df_verifier_depot.to_csv(verifier_depot_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist Admin V√©rifier D√©p√¥t: {len(df_verifier_depot)} formations (D√©p√¥t brouillon)")
                    else:
                        print(f"   ‚ö† No formations found with 'D√©p√¥t brouillon' status")

                # Checklist 4: √âquipe commercial - Manque signatures (from Potentiel column)
                if 'Potentiel' in df.columns:
                    df_manque_signatures = df[df['Potentiel'] == 'Manque signatures'].copy()
                    if not df_manque_signatures.empty:
                        manque_signatures_path = os.path.join(checklist_dir, 'checklist_√©quipe_commercial.csv')
                        df_manque_signatures.to_csv(manque_signatures_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist √âquipe Commercial: {len(df_manque_signatures)} formations (Manque signatures)")
                    else:
                        print(f"   ‚ö† No formations found with 'Manque signatures' status")

                # Checklist 5: D√©p√¥t que le client doit effectuer - D√©p√¥t irr√©alisable faute de mandat (from Potentiel column)
                if 'Potentiel' in df.columns:
                    df_depot_client = df[df['Potentiel'] == 'D√©p√¥t irr√©alisable faute de mandat'].copy()
                    if not df_depot_client.empty:
                        depot_client_path = os.path.join(checklist_dir, 'd√©p√¥t_que_le_client_doit_effectuer.csv')
                        df_depot_client.to_csv(depot_client_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist D√©p√¥t Client: {len(df_depot_client)} formations (D√©p√¥t irr√©alisable faute de mandat)")
                    else:
                        print(f"   ‚ö† No formations found with 'D√©p√¥t irr√©alisable faute de mandat' status")

                # Checklist 6: Prochaine facturation - Facturations en retard
                if 'Prochaine facturation' in df.columns:
                    from datetime import datetime

                    # Convert 'Prochaine facturation' to datetime
                    df['Prochaine facturation'] = pd.to_datetime(df['Prochaine facturation'], errors='coerce')

                    # Get today's date (without time)
                    today = pd.Timestamp.now().normalize()

                    # Filter rows where 'Prochaine facturation' is before today (and not NaT)
                    df_facturation_retard = df[
                        (df['Prochaine facturation'].notna()) &
                        (df['Prochaine facturation'] < today)
                    ].copy()

                    if not df_facturation_retard.empty:
                        facturation_retard_path = os.path.join(checklist_dir, 'checklist_facturation_en_retard.csv')
                        df_facturation_retard.to_csv(facturation_retard_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist Facturation en Retard: {len(df_facturation_retard)} formations (Date de facturation d√©pass√©e)")

                        # Show some details
                        oldest_date = df_facturation_retard['Prochaine facturation'].min()
                        print(f"      Date la plus ancienne: {oldest_date.strftime('%Y-%m-%d')}")
                    else:
                        print(f"   ‚úì No overdue invoices - all up to date!")

                # Checklist 7: Tr√©sorerie en retard - Factur√© depuis plus de 2 mois
                if 'R√©√©l' in df.columns and 'DATE DE DEBUT FORMATION' in df.columns:
                    from datetime import datetime, timedelta

                    # Convert 'DATE DE DEBUT FORMATION' to datetime
                    df['DATE DE DEBUT FORMATION'] = pd.to_datetime(df['DATE DE DEBUT FORMATION'], errors='coerce')

                    # Get today's date (without time)
                    today = pd.Timestamp.now().normalize()

                    # Calculate date 2 months ago (60 days)
                    two_months_ago = today - timedelta(days=90)

                    # Filter rows where 'R√©√©l' is 'Factur√©' AND 'DATE DE DEBUT FORMATION' is more than 3 months old
                    df_tresorerie_retard = df[
                        (df['R√©√©l'] == 'Factur√©') &
                        (df['DATE DE DEBUT FORMATION'].notna()) &
                        (df['DATE DE DEBUT FORMATION'] < two_months_ago)
                    ].copy()

                    if not df_tresorerie_retard.empty:
                        tresorerie_retard_path = os.path.join(checklist_dir, 'tresorerie_en_retard.csv')
                        df_tresorerie_retard.to_csv(tresorerie_retard_path, index=False, encoding='utf_8_sig', sep=';')
                        print(
                            f"   ‚úì Checklist Tr√©sorerie en Retard: {len(df_tresorerie_retard)} formations (Factur√© depuis plus de 2 mois)")

                        # Show some details
                        oldest_date = df_tresorerie_retard['DATE DE DEBUT FORMATION'].min()
                        print(f"      Date de d√©but la plus ancienne: {oldest_date.strftime('%Y-%m-%d')}")
                    else:
                        print(f"   ‚úì No overdue treasury items - all up to date!")

                # =============================================================================
                # CREATE CHECKLIST RECAP
                # =============================================================================

                print(f"\n[RECAP] Creating checklist recap...")

                # Get all CSV files in checklist directory (excluding recap file)
                all_checklist_files = glob.glob(os.path.join(checklist_dir, '*.csv'))
                checklist_files = [f for f in all_checklist_files if not f.endswith('checklist_recap.csv')]

                if checklist_files:
                    recap_data = []

                    for checklist_file in checklist_files:
                        try:
                            # Read CSV to count rows
                            df_checklist = pd.read_csv(checklist_file, encoding='utf_8_sig', sep=';')

                            # Get filename without path
                            filename = os.path.basename(checklist_file)

                            # Count rows (excluding header)
                            row_count = len(df_checklist)

                            recap_data.append({
                                'Fichier': filename,
                                'Nombre de lignes': row_count
                            })

                        except Exception as e:
                            print(f"   ‚ö† Error reading {filename}: {e}")

                    # Create recap DataFrame
                    if recap_data:
                        df_recap = pd.DataFrame(recap_data)

                        # Sort by number of lines descending
                        df_recap = df_recap.sort_values('Nombre de lignes', ascending=False)

                        # Add total row
                        total_row = pd.DataFrame({
                            'Fichier': ['TOTAL'],
                            'Nombre de lignes': [df_recap['Nombre de lignes'].sum()]
                        })
                        df_recap = pd.concat([df_recap, total_row], ignore_index=True)

                        # Save recap CSV
                        recap_path = os.path.join(checklist_dir, 'checklist_recap.csv')
                        df_recap.to_csv(recap_path, index=False, encoding='utf_8_sig', sep=';')

                        print(f"   ‚úì Checklist recap created: {len(checklist_files)} checklists analyzed")
                        print(f"   ‚úì Total formations across all checklists: {df_recap['Nombre de lignes'].iloc[-1]}")

                        # Display recap
                        print(f"\n   Recap:")
                        for _, row in df_recap.iterrows():
                            print(f"   {row['Fichier']}: {int(row['Nombre de lignes'])} ligne(s)")

        except Exception as e:
            print(f"‚úó Error segmenting {csv_file}: {e}")
            import traceback
            traceback.print_exc()


# =============================================================================
# STEP 4: CREATE VISUALIZATIONS
# =============================================================================

def create_payment_visualization(dest_dir, graph_dir):
    """
    Create a bar chart showing payment totals by Vague for R√©el status only
    """
    import matplotlib.pyplot as plt
    import numpy as np

    try:
        # Read the transformed data
        summary_path = os.path.join(dest_dir, 'Donn√©es_Transform√©es.csv')
        if not os.path.exists(summary_path):
            print("‚ö† Warning: Donn√©es_Transform√©es.csv not found. Skipping visualization.")
            return

        df_summary = pd.read_csv(summary_path, encoding='utf_8_sig', sep=';')

        # Debug: Print column names to verify correct parsing
        print(f"Debug - Columns in summary: {df_summary.columns.tolist()[:5]}")  # Print first 5 columns

        # Filter only 'R√©el' or 'R√©√©l' status (handle accent variations)
        df_reel = df_summary[df_summary['√âTAT'].isin(['R√©el', 'R√©√©l'])].copy()

        if df_reel.empty:
            print("‚ö† Warning: No 'R√©el' or 'R√©√©l' data found for visualization.")
            return

        # Get unique vagues
        vagues = df_reel['Vague'].unique()

        # Prepare data for plotting
        payment_columns = ['Total_PAIEMENT 1', 'Total_PAIEMENT 2', 'Total_PAIEMENT 3']

        # Create the plot
        fig, ax = plt.subplots(figsize=(12, 7))

        # Set the width of bars and positions
        x = np.arange(len(payment_columns))
        width = 0.25

        # Define colors for each vague
        colors = ['#2E86AB', '#A23B72', '#F18F01']  # Blue, Purple, Orange

        # Collect payment data for totals
        payment_totals = [0, 0, 0]

        # Plot bars for each vague
        for i, vague in enumerate(vagues):
            vague_data = df_reel[df_reel['Vague'] == vague]

            if not vague_data.empty:
                values = [
                    vague_data['Total_PAIEMENT 1'].sum(),
                    vague_data['Total_PAIEMENT 2'].sum(),
                    vague_data['Total_PAIEMENT 3'].sum()
                ]

                # Add to totals
                for j in range(3):
                    payment_totals[j] += values[j]

                offset = width * (i - len(vagues) / 2 + 0.5)
                bars = ax.bar(x + offset, values, width, label=f'Vague {vague}',
                              color=colors[i % len(colors)])

                # Add value labels on bars
                for bar in bars:
                    height = bar.get_height()
                    if height > 0:
                        ax.text(bar.get_x() + bar.get_width() / 2., height,
                                f'{height:,.0f}‚Ç¨',
                                ha='center', va='bottom', fontsize=9, rotation=0)

        # Customize the plot
        ax.set_xlabel('Type de Paiement', fontsize=12, fontweight='bold')
        ax.set_ylabel('Montant Total (‚Ç¨)', fontsize=12, fontweight='bold')
        ax.set_title('Paiements R√©els par Vague et Type de Paiement',
                     fontsize=14, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(['Paiement 1', 'Paiement 2', 'Paiement 3'])
        ax.legend(title='Cycles', fontsize=10, title_fontsize=11)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Format y-axis to show currency
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}‚Ç¨'))

        # Calculate and display totals
        total_global = sum(payment_totals)
        fig.text(0.5, 0.02,
                 f'TOTAL: Paiement 1: {payment_totals[0]:,.0f}‚Ç¨ | Paiement 2: {payment_totals[1]:,.0f}‚Ç¨ | Paiement 3: {payment_totals[2]:,.0f}‚Ç¨ | Global: {total_global:,.0f}‚Ç¨',
                 ha='center', fontsize=11, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))

        plt.tight_layout(rect=[0, 0.04, 1, 1])  # Add space at bottom for text

        # Create graph directory if it doesn't exist
        os.makedirs(graph_dir, exist_ok=True)

        # Save the figure
        output_path = os.path.join(graph_dir, 'Paiements_par_Vague.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"\nüìä Visualization created: {output_path}")

    except Exception as e:
        print(f"‚úó Error creating visualization: {e}")
        import traceback
        traceback.print_exc()


def create_status_count_visualization(dest_dir, graph_dir):
    """
    Create a 3x3 grid showing Count by √âTAT and Vague
    """
    import matplotlib.pyplot as plt
    import numpy as np

    try:
        # Read the transformed data
        summary_path = os.path.join(dest_dir, 'Donn√©es_Transform√©es.csv')
        if not os.path.exists(summary_path):
            print("‚ö† Warning: Donn√©es_Transform√©es.csv not found. Skipping visualization.")
            return

        df_summary = pd.read_csv(summary_path, encoding='utf_8_sig', sep=';')

        # Get unique vagues and √©tats
        vagues = sorted(df_summary['Vague'].unique())
        etats = ['R√©√©l', 'Pr√©visionnel', 'Potentiel']  # Fixed order

        # Create pivot table for the data
        pivot_data = []
        for vague in vagues:
            row = []
            for etat in etats:
                mask = (df_summary['Vague'] == vague) & (df_summary['√âTAT'] == etat)
                count = df_summary[mask]['Count'].sum() if mask.any() else 0
                row.append(count)
            pivot_data.append(row)

        pivot_data = np.array(pivot_data)

        # Create the plot
        fig, ax = plt.subplots(figsize=(10, 8))

        # Create heatmap-style bar chart
        x = np.arange(len(etats))
        width = 0.25
        colors = ['#2E86AB', '#A23B72', '#F18F01']  # Blue, Purple, Orange

        for i, vague in enumerate(vagues):
            offset = width * (i - len(vagues) / 2 + 0.5)
            bars = ax.bar(x + offset, pivot_data[i], width,
                          label=f'{vague}', color=colors[i % len(colors)])

            # Add value labels on bars
            for j, bar in enumerate(bars):
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2., height,
                            f'{int(height)}',
                            ha='center', va='bottom', fontsize=11, fontweight='bold')

        # Customize the plot
        ax.set_xlabel('√âtat de Facturation', fontsize=13, fontweight='bold')
        ax.set_ylabel('Nombre de Formations', fontsize=13, fontweight='bold')
        ax.set_title('R√©partition des Formations par Vague et √âtat',
                     fontsize=15, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(etats, fontsize=11)

        # Set y-axis limit to add space for legend (30% higher than max value)
        max_value = pivot_data.max()
        ax.set_ylim(0, max_value * 1.3)

        ax.legend(title='Cycles', fontsize=11, title_fontsize=12, loc='upper right')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Calculate totals for each √©tat across all vagues
        total_reel = int(pivot_data[:, 0].sum())
        total_previsionnel = int(pivot_data[:, 1].sum())
        total_potentiel = int(pivot_data[:, 2].sum())
        total_global = total_reel + total_previsionnel + total_potentiel

        # Add total recap at the bottom
        fig.text(0.5, 0.02,
                 f'TOTAL: R√©√©l: {total_reel} | Pr√©visionnel: {total_previsionnel} | Potentiel: {total_potentiel} | Global: {total_global}',
                 ha='center', fontsize=11, fontweight='bold',
                 bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))

        plt.tight_layout(rect=[0, 0.04, 1, 1])  # Add space at bottom for text

        # Create graph directory if it doesn't exist
        os.makedirs(graph_dir, exist_ok=True)

        # Save the figure
        output_path = os.path.join(graph_dir, 'Statut_Formations_par_Vague.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"üìä Status visualization created: {output_path}")

        # Print summary table
        print("\nüìã Summary Table:")
        print(f"{'Vague':<12} {'R√©√©l':<12} {'Pr√©visionnel':<15} {'Potentiel':<12} {'Total':<10}")
        print("-" * 65)
        for i, vague in enumerate(vagues):
            total = pivot_data[i].sum()
            print(
                f"{vague:<12} {int(pivot_data[i][0]):<12} {int(pivot_data[i][1]):<15} {int(pivot_data[i][2]):<12} {int(total):<10}")
        print("-" * 65)
        grand_total = pivot_data.sum()
        print(
            f"{'TOTAL':<12} {int(pivot_data[:, 0].sum()):<12} {int(pivot_data[:, 1].sum()):<15} {int(pivot_data[:, 2].sum()):<12} {int(pivot_data.sum()):<10}")

    except Exception as e:
        print(f"‚úó Error creating status visualization: {e}")
        import traceback
        traceback.print_exc()


def create_ca_visualization_by_vague(dest_dir, graph_dir):
    """
    Create a single bar chart showing CA R√©√©l, CA Pr√©visionnel, and CA Potentiel for all Vagues
    3 categories (x-axis) x 3 vagues (color-coded bars)
    """
    import matplotlib.pyplot as plt
    import numpy as np

    try:
        # Read the transformed data
        summary_path = os.path.join(dest_dir, 'Donn√©es_Transform√©es.csv')
        if not os.path.exists(summary_path):
            print("‚ö† Warning: Donn√©es_Transform√©es.csv not found. Skipping visualization.")
            return

        df_summary = pd.read_csv(summary_path, encoding='utf_8_sig', sep=';')

        # Get unique vagues
        vagues = sorted(df_summary['Vague'].unique())

        # Create graph directory if it doesn't exist
        os.makedirs(graph_dir, exist_ok=True)

        # Prepare data: 3 CA categories
        categories = ['CA R√©√©l', 'CA Pr√©visionnel', 'CA Potentiel']

        # Create pivot data structure
        ca_data = []
        for vague in vagues:
            df_vague = df_summary[df_summary['Vague'] == vague]
            ca_reel = df_vague['CA_R√©√©l'].sum()
            ca_previsionnel = df_vague['CA_Pr√©visionnel'].sum()
            ca_potentiel = df_vague['CA_Potentiel'].sum()
            ca_data.append([ca_reel, ca_previsionnel, ca_potentiel])

        ca_data = np.array(ca_data)

        # Create the plot
        fig, ax = plt.subplots(figsize=(12, 8))

        # Set the width of bars and positions
        x = np.arange(len(categories))
        width = 0.25

        # Define colors for each vague
        colors = ['#2E86AB', '#A23B72', '#F18F01']  # Blue, Purple, Orange

        # Plot bars for each vague
        for i, vague in enumerate(vagues):
            offset = width * (i - len(vagues) / 2 + 0.5)
            bars = ax.bar(x + offset, ca_data[i], width,
                          label=f'{vague}', color=colors[i % len(colors)])

            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                if height > 0:
                    ax.text(bar.get_x() + bar.get_width() / 2., height,
                            f'{height:,.0f}‚Ç¨',
                            ha='center', va='bottom', fontsize=10, fontweight='bold')

        # Customize the plot
        ax.set_xlabel('Cat√©gorie de Chiffre d\'Affaires', fontsize=13, fontweight='bold')
        ax.set_ylabel('Montant (‚Ç¨)', fontsize=13, fontweight='bold')
        ax.set_title('Chiffre d\'Affaires par Cat√©gorie et par Vague',
                     fontsize=15, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(categories, fontsize=11)

        # Set y-axis limit to add space for legend
        max_value = ca_data.max()
        ax.set_ylim(0, max_value * 1.25)

        ax.legend(title='Cycles', fontsize=11, title_fontsize=12, loc='upper left')
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Format y-axis to show currency
        ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{int(x):,}‚Ç¨'))

        # Add total CA at the bottom
        total_ca_reel = ca_data[:, 0].sum()
        total_ca_previsionnel = ca_data[:, 1].sum()
        total_ca_potentiel = ca_data[:, 2].sum()
        total_global = total_ca_reel + total_ca_previsionnel + total_ca_potentiel

        # Add text below the chart with proper spacing
        fig.text(0.5, 0.01,
                 f'Total Global: {total_global:,.0f}‚Ç¨',
                 ha='center', fontsize=11, fontweight='bold')
        fig.text(0.5, -0.02,
                 f'(R√©√©l: {total_ca_reel:,.0f}‚Ç¨  |  Pr√©visionnel: {total_ca_previsionnel:,.0f}‚Ç¨  |  Potentiel: {total_ca_potentiel:,.0f}‚Ç¨)',
                 ha='center', fontsize=9)

        plt.tight_layout(rect=[0, 0.03, 1, 1])  # Add space at bottom for text

        # Save the figure
        output_path = os.path.join(graph_dir, 'CA_par_Cat√©gorie_Toutes_Vagues.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"üìä CA visualization created: {output_path}")

    except Exception as e:
        print(f"‚úó Error creating CA visualization: {e}")
        import traceback
        traceback.print_exc()


def create_intermediary_status_visualization(dest_dir, graph_dir):
    """
    Create visualizations for intermediary statuses per vague and per category
    """
    import matplotlib.pyplot as plt
    import numpy as np

    try:
        # Read the intermediary status data
        intermediary_path = os.path.join(dest_dir, 'Statuts_Interm√©diaires.csv')
        if not os.path.exists(intermediary_path):
            print("‚ö† Warning: Statuts_Interm√©diaires.csv not found. Skipping visualization.")
            return

        df_inter = pd.read_csv(intermediary_path, encoding='utf_8_sig', sep=';')

        # Create graph directory if it doesn't exist
        os.makedirs(graph_dir, exist_ok=True)

        # Define status categories
        status_categories = {
            'R√©√©l': ['PEC accord√©', 'Factur√©', 'Encaiss√©'],
            'Pr√©visionnel': ['ATT DE PEC', 'ATT DE PEC R1', 'ATT DE PEC R2', 'ATT DE PEC R3',
                             'ATT DE PEC R4', 'ATT DE PEC R5', 'ATT DE PEC R6', 'ATT DE PEC R7',
                             'ATT DE PEC R8', 'ATT DE PEC R9', 'ATT DE PEC R10', 'ATT DE PEC R11',
                             'ATT DE PEC R12'],
            'Potentiel': ['Signature bi-parti √† d√©poser', 'D√©p√¥t brouillon', 'Manque info', 'Manque documents']
        }

        # Get unique vagues
        vagues = sorted(df_inter['Vague'].unique())
        colors = ['#2E86AB', '#A23B72', '#F18F01']  # Blue, Purple, Orange

        # Create one visualization per category
        for category, statuses in status_categories.items():
            # Filter data for this category
            df_category = df_inter[df_inter['√âTAT'] == category]

            if df_category.empty:
                continue

            # Get statuses that have values > 0
            active_statuses = []
            for status in statuses:
                if status in df_category.columns and df_category[status].sum() > 0:
                    active_statuses.append(status)

            if not active_statuses:
                continue

            # Create the plot
            fig, ax = plt.subplots(figsize=(14, 8))

            # Set the width of bars and positions
            x = np.arange(len(active_statuses))
            width = 0.25

            # Calculate totals for each status
            status_totals = {status: 0 for status in active_statuses}

            # Plot bars for each vague
            for i, vague in enumerate(vagues):
                df_vague = df_category[df_category['Vague'] == vague]

                if not df_vague.empty:
                    values = [df_vague[status].sum() for status in active_statuses]

                    # Add to totals
                    for j, status in enumerate(active_statuses):
                        status_totals[status] += values[j]

                    offset = width * (i - len(vagues) / 2 + 0.5)
                    bars = ax.bar(x + offset, values, width,
                                  label=f'{vague}', color=colors[i % len(colors)])

                    # Add value labels on bars
                    for bar in bars:
                        height = bar.get_height()
                        if height > 0:
                            ax.text(bar.get_x() + bar.get_width() / 2., height,
                                    f'{int(height)}',
                                    ha='center', va='bottom', fontsize=9, fontweight='bold')

            # Customize the plot
            ax.set_xlabel('Statut Interm√©diaire', fontsize=12, fontweight='bold')
            ax.set_ylabel('Nombre de Formations', fontsize=12, fontweight='bold')
            ax.set_title(f'Statuts Interm√©diaires - {category}',
                         fontsize=14, fontweight='bold', pad=20)
            ax.set_xticks(x)
            ax.set_xticklabels(active_statuses, rotation=45, ha='right', fontsize=9)
            ax.legend(title='Cycles', fontsize=10, title_fontsize=11)
            ax.grid(axis='y', alpha=0.3, linestyle='--')

            # Add totals at the bottom
            total_global = sum(status_totals.values())
            totals_text = " | ".join([f"{status}: {int(count)}" for status, count in status_totals.items()])
            fig.text(0.5, 0.02,
                     f'TOTAL ({category}): {totals_text} | Global: {int(total_global)}',
                     ha='center', fontsize=10, fontweight='bold',
                     bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgray', alpha=0.8))

            plt.tight_layout(rect=[0, 0.04, 1, 1])  # Add space at bottom for text

            # Save the figure
            category_safe = category.replace('√©', 'e').replace(' ', '_')
            output_path = os.path.join(graph_dir, f'Statuts_Interm√©diaires_{category_safe}.png')
            plt.savefig(output_path, dpi=300, bbox_inches='tight')
            plt.close()

            print(f"üìä Intermediary status visualization created for {category}: {output_path}")

    except Exception as e:
        print(f"‚úó Error creating intermediary status visualization: {e}")
        import traceback
        traceback.print_exc()


def create_promo_visualization(dest_dir, graph_dir):
    """
    Create single visualization with 3 bars (one per Vague) showing PROMO breakdown
    """
    import matplotlib.pyplot as plt
    import numpy as np

    try:
        # Read the PROMO data
        promo_path = os.path.join(dest_dir, 'PROMO_R√©√©l_par_Vague.csv')
        if not os.path.exists(promo_path):
            print("‚ö† Warning: PROMO_R√©√©l_par_Vague.csv not found. Skipping visualization.")
            return

        df_promo = pd.read_csv(promo_path, encoding='utf_8_sig', sep=';')

        # Create graph directory if it doesn't exist
        os.makedirs(graph_dir, exist_ok=True)

        # Get unique vagues and promos
        vagues = sorted(df_promo['Vague'].unique())
        all_promos = sorted(df_promo['PROMO'].unique())

        # Calculate totals per vague
        vague_totals = {vague: df_promo[df_promo['Vague'] == vague]['Count'].sum()
                        for vague in vagues}

        # Create pivot data structure: rows=vagues, columns=promos
        promo_data = []
        for vague in vagues:
            row = []
            for promo in all_promos:
                df_subset = df_promo[(df_promo['Vague'] == vague) & (df_promo['PROMO'] == promo)]
                count = df_subset['Count'].sum() if not df_subset.empty else 0
                row.append(count)
            promo_data.append(row)

        promo_data = np.array(promo_data)

        # Calculate global total and percentage for each PROMO (across all vagues)
        global_total = promo_data.sum()
        promo_global_totals = {promo: promo_data[:, i].sum() for i, promo in enumerate(all_promos)}
        promo_percentages = {promo: (count / global_total * 100) if global_total > 0 else 0
                            for promo, count in promo_global_totals.items()}

        # Create the plot
        fig, ax = plt.subplots(figsize=(12, 8))

        # Set positions for the 3 vague bars
        x = np.arange(len(vagues))
        width = 0.6

        # Define colors for each PROMO (will be used in stacked bars and legend)
        promo_colors = plt.cm.Set3(np.linspace(0, 1, len(all_promos)))

        # Create stacked bars
        bottom = np.zeros(len(vagues))

        for i, promo in enumerate(all_promos):
            values = promo_data[:, i]
            # Create label with percentage for legend
            promo_label = f'{promo} ({promo_percentages[promo]:.1f}%)'
            bars = ax.bar(x, values, width, label=promo_label, bottom=bottom, color=promo_colors[i])

            # Add value labels on bars (only if value > 0)
            for j, (bar, val) in enumerate(zip(bars, values)):
                if val > 0:
                    height = bar.get_height()
                    y_pos = bottom[j] + height / 2

                    # Calculate percentage
                    percentage = (val / vague_totals[vagues[j]]) * 100

                    # For EPR and EC, show count and percentage
                    if promo in ['EPR', 'EC']:
                        label_text = f'{int(val)}\n({percentage:.1f}%)'
                        fontsize = 9
                    else:
                        label_text = f'{int(val)}'
                        fontsize = 9

                    ax.text(bar.get_x() + bar.get_width() / 2., y_pos,
                            label_text,
                            ha='center', va='center', fontsize=fontsize, fontweight='bold')

            bottom += values

        # Customize the plot
        ax.set_xlabel('Vague', fontsize=13, fontweight='bold')
        ax.set_ylabel('Nombre de Formations R√©elles', fontsize=13, fontweight='bold')
        ax.set_title('R√©partition des Formations R√©elles par PROMO et par Vague',
                     fontsize=15, fontweight='bold', pad=20)
        ax.set_xticks(x)
        ax.set_xticklabels(vagues, fontsize=12)

        # Add legend with percentages (% across all vagues)
        ax.legend(title='PROMO (% global sur 3 vagues)', fontsize=8, title_fontsize=9,
                 loc='upper right', ncol=2, framealpha=0.95)
        ax.grid(axis='y', alpha=0.3, linestyle='--')

        # Add totals for each vague at the bottom
        totals_text = "  |  ".join([f"{vague}: {int(promo_data[i].sum())} formations"
                                    for i, vague in enumerate(vagues)])
        fig.text(0.5, 0.01, totals_text,
                 ha='center', fontsize=10, fontweight='bold')

        plt.tight_layout(rect=[0, 0.03, 1, 1])

        # Save the figure
        output_path = os.path.join(graph_dir, 'PROMO_Reel_par_Vague.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()

        print(f"üìä PROMO visualization created: {output_path}")

    except Exception as e:
        print(f"‚úó Error creating PROMO visualization: {e}")
        import traceback
        traceback.print_exc()


# =============================================================================
# MAIN EXECUTION
# =============================================================================

if __name__ == "__main__":
    print("=" * 70)
    print("XLSX TO CSV CONVERTER - VAGUE SEGMENTATION")
    print("=" * 70)

    # Step 1 & 2: Convert Excel to CSV with encoding
    print("\n[STEP 1-2] Converting XLSX to CSV with proper encoding...")
    converted_files = convert_xlsx_to_csv(
        files_to_convert,
        source_directory,
        destination_directory
    )

    # Step 3: Segment by Vague
    print("\n[STEP 3] Segmenting data by Vague (cycles)...")
    segment_by_vague(converted_files, destination_directory)

    # Step 4: Create visualizations
    print("\n[STEP 4] Creating visualizations...")
    graph_directory = os.path.join(source_directory, 'Graphiques')
    os.makedirs(graph_directory, exist_ok=True)
    create_payment_visualization(destination_directory, graph_directory)
    create_status_count_visualization(destination_directory, graph_directory)
    create_ca_visualization_by_vague(destination_directory, graph_directory)
    create_intermediary_status_visualization(destination_directory, graph_directory)
    create_promo_visualization(destination_directory, graph_directory)

    # Step 5: Convert checklist CSVs to Excel
    print("\n[STEP 5] Converting checklist CSVs to Excel...")
    checklist_files_to_convert = [
        'checklist_√©quipe_commercial.csv',
        'checklist_admin_d√©p√¥t_initial.csv',
        'checklist_admin_v√©rifier_d√©p√¥t.csv',
        'checklist_cindy.csv',
        'checklist_facturation_en_retard.csv',
        'd√©p√¥t_que_le_client_doit_effectuer.csv',
        'tresorerie_en_retard.csv'
    ]

    for csv_file in checklist_files_to_convert:
        csv_path = os.path.join(destination_directory, csv_file)
        if os.path.exists(csv_path):
            try:
                df = pd.read_csv(csv_path, sep=';', encoding='utf-8-sig')
                excel_file = csv_file.replace('.csv', '.xlsx')
                excel_path = os.path.join(destination_directory, excel_file)
                df.to_excel(excel_path, index=False, engine='openpyxl')
                print(f"   ‚úì Converted: {csv_file} ‚Üí {excel_file}")
            except Exception as e:
                print(f"   ‚úó Error converting {csv_file}: {e}")

    # Update recap to include all checklist files
    print("\n[UPDATING RECAP] Adding new checklist files...")
    recap_path = os.path.join(destination_directory, 'checklist_recap.xlsx')
    if os.path.exists(recap_path):
        try:
            df_recap = pd.read_excel(recap_path, engine='openpyxl')
            # Remove TOTAL row
            df_recap = df_recap[df_recap['Fichier'] != 'TOTAL']

            # Add missing files
            new_files = [
                ('depot_que_le_client_doit_effectuer.csv', 'd√©p√¥t_que_le_client_doit_effectuer.csv'),
                ('tresorerie_en_retard.csv', 'tresorerie_en_retard.csv')
            ]

            for xlsx_name, csv_name in new_files:
                csv_path = os.path.join(destination_directory, csv_name)
                if os.path.exists(csv_path) and xlsx_name not in df_recap['Fichier'].values:
                    try:
                        df_file = pd.read_csv(csv_path, sep=';', encoding='utf-8-sig')
                        count = len(df_file)
                        new_row = pd.DataFrame({'Fichier': [xlsx_name], 'Nombre de lignes': [count]})
                        df_recap = pd.concat([df_recap, new_row], ignore_index=True)
                        print(f"   ‚úì Added to recap: {xlsx_name} ({count} lines)")
                    except Exception as e:
                        print(f"   ‚úó Error adding {xlsx_name}: {e}")

            # Recalculate TOTAL
            total_lines = df_recap['Nombre de lignes'].sum()
            total_row = pd.DataFrame({'Fichier': ['TOTAL'], 'Nombre de lignes': [total_lines]})
            df_recap = pd.concat([df_recap, total_row], ignore_index=True)

            # Save updated recap
            df_recap.to_excel(recap_path, index=False, engine='openpyxl')
            print(f"   ‚úì Recap updated with {len(df_recap)-1} checklists")
        except Exception as e:
            print(f"   ‚úó Error updating recap: {e}")

    print("\n" + "=" * 70)
    print("‚úì PROCESSING COMPLETE")
    print("=" * 70)
    print(f"Output location: {destination_directory}")
    print(f"Graph location: {graph_directory}")
